{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader as PygDataLoader\n",
    "from torch.utils.data import SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch_geometric.data import DataLoader\n",
    "from dataloader import NCaltech101Best\n",
    "from model import GraphRes, SimpleNet\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "batch_size= 16\n",
    "lr=         10e-3 # decreases by 10 after each 20 epochs\n",
    "loss=       th.nn.CrossEntropyLoss\n",
    "batchsize=  16\n",
    "K=          10 # data subsampling\n",
    "nclasses=   100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loading classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "100%|██████████| 101/101 [00:00<00:00, 21346.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example datapoint: Data(x=[1000, 1], y=8, pos=[1000, 3], edge_index=[2, 5911], edge_attr=[5911, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SHAPE: tensor([240, 180,   3], device='cuda:0')\n",
      "loading classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "100%|██████████| 101/101 [00:00<00:00, 21500.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing epoch 0\n",
      "234 iterations in total:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/434 [00:00<?, ?it/s]/home/nataxcan/miniconda3/envs/dl_proj2/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n",
      "precision so far is 0.08: 100%|█████████▉| 433/434 [00:40<00:00, 10.59it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (16000) must match the existing size (3000) at non-singleton dimension 0.  Target sizes: [16000, 1].  Tensor sizes: [3000, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m     86\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcomputing epoch\u001b[39m\u001b[39m\"\u001b[39m, epoch)\n\u001b[0;32m---> 87\u001b[0m     losses \u001b[39m=\u001b[39m train()\n\u001b[1;32m     88\u001b[0m     all_losses\u001b[39m.\u001b[39mappend(losses)\n",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 52\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     54\u001b[0m \u001b[39m# weight updates\u001b[39;00m\n\u001b[1;32m     55\u001b[0m y \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(data\u001b[39m.\u001b[39my, num_classes\u001b[39m=\u001b[39m\u001b[39m101\u001b[39m)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mFloatTensor)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj2/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/programming_stuff/deep_learning_project/from_scratch/model.py:112\u001b[0m, in \u001b[0;36mSimpleNet.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    110\u001b[0m data\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(data\u001b[39m.\u001b[39mx)\n\u001b[1;32m    111\u001b[0m \u001b[39m# print(\"x before:\", x, x.shape)\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool1(data\u001b[39m.\u001b[39;49mx, pos\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mpos[:, :\u001b[39m2\u001b[39;49m], batch\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mbatch)\n\u001b[1;32m    113\u001b[0m \u001b[39m# print(\"x after:\", x, x.shape)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m# print(\"FEATURES:\", x, x.shape)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor_split(torch\u001b[39m.\u001b[39mflatten(x), \u001b[39m16\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj2/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/programming_stuff/deep_learning_project/from_scratch/layers.py:18\u001b[0m, in \u001b[0;36mMaxPoolingX.forward\u001b[0;34m(self, x, pos, batch)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor, pos: torch\u001b[39m.\u001b[39mTensor, batch: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m             ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mLongTensor, torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], Data]:\n\u001b[1;32m     17\u001b[0m     cluster \u001b[39m=\u001b[39m voxel_grid(pos, batch\u001b[39m=\u001b[39mbatch, size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvoxel_size)\n\u001b[0;32m---> 18\u001b[0m     x, _ \u001b[39m=\u001b[39m max_pool_x(cluster, x, batch, size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize)\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj2/lib/python3.9/site-packages/torch_geometric/nn/pool/max_pool.py:47\u001b[0m, in \u001b[0;36mmax_pool_x\u001b[0;34m(cluster, x, batch, size)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mif\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(batch\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mitem()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m _max_pool_x(cluster, x, batch_size \u001b[39m*\u001b[39;49m size), \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     49\u001b[0m cluster, perm \u001b[39m=\u001b[39m consecutive_cluster(cluster)\n\u001b[1;32m     50\u001b[0m x \u001b[39m=\u001b[39m _max_pool_x(cluster, x)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj2/lib/python3.9/site-packages/torch_geometric/nn/pool/max_pool.py:16\u001b[0m, in \u001b[0;36m_max_pool_x\u001b[0;34m(cluster, x, size)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_max_pool_x\u001b[39m(\n\u001b[1;32m     12\u001b[0m     cluster: Tensor,\n\u001b[1;32m     13\u001b[0m     x: Tensor,\n\u001b[1;32m     14\u001b[0m     size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter(x, cluster, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, dim_size\u001b[39m=\u001b[39;49msize, reduce\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj2/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:97\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m src\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     93\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe usage of `scatter(reduce=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mreduce\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m                       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcan be accelerated via the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch-scatter\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m                       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m package, but it was not found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m     index \u001b[39m=\u001b[39m broadcast(index, src, dim)\n\u001b[1;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39mnew_zeros(size)\u001b[39m.\u001b[39mscatter_reduce_(\n\u001b[1;32m     99\u001b[0m         dim, index, src, reduce\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m{\u001b[39;00mreduce\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, include_self\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m torch_scatter\u001b[39m.\u001b[39mscatter(src, index, dim, dim_size\u001b[39m=\u001b[39mdim_size,\n\u001b[1;32m    102\u001b[0m                              reduce\u001b[39m=\u001b[39mreduce)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj2/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:21\u001b[0m, in \u001b[0;36mbroadcast\u001b[0;34m(src, ref, dim)\u001b[0m\n\u001b[1;32m     19\u001b[0m size \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m ref\u001b[39m.\u001b[39mdim()\n\u001b[1;32m     20\u001b[0m size[dim] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39;49mview(size)\u001b[39m.\u001b[39;49mexpand_as(ref)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (16000) must match the existing size (3000) at non-singleton dimension 0.  Target sizes: [16000, 1].  Tensor sizes: [3000, 1]"
     ]
    }
   ],
   "source": [
    "# Set device to use for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"using device:\", device)\n",
    "\n",
    "# Initialize dataset and data loader\n",
    "dataset = NCaltech101Best('./data/storage/', mode='train')\n",
    "# sampler = SequentialSampler(dataset)\n",
    "# loader = DataLoader(dataset, batch_size=16, sampler=sampler)\n",
    "loader = PygDataLoader(dataset, batch_size=16, shuffle=True)\n",
    "print(\"example datapoint:\", dataset.get(0))\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model_input_shape = th.tensor((240, 180) + (3, ), device=device)\n",
    "print(\"INPUT SHAPE:\", model_input_shape)\n",
    "model = SimpleNet(model_input_shape, 101).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.2)\n",
    "\n",
    "# for testing\n",
    "test_dataset = NCaltech101Best('./data/storage/', mode='test')\n",
    "test_loader = PygDataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "def run_model_test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            pred = out.max(dim=1)[1]\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "acc = []\n",
    "test_acc = []\n",
    "# Define training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    i = 0\n",
    "    print(\"234 iterations in total:\")\n",
    "    progbar = tqdm(loader)\n",
    "    for data in progbar:\n",
    "\n",
    "        # inference\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        # weight updates\n",
    "        y = F.one_hot(data.y, num_classes=101).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(output, y)\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # precision logging\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "        i += 1\n",
    "        # if i % 50 == 0:\n",
    "        #     print(correct, i*16)\n",
    "        #     print(correct / (i*16))\n",
    "        # if i % 20 == 0 and i != 0:\n",
    "        #     print((i, round(correct / (i*16), 2)), end='->')\n",
    "        # if i % 100 == 0 and i != 0:\n",
    "        #     print(\"\\n\", end='')\n",
    "        precision = round(correct / (i*16), 2)\n",
    "        progbar.set_description(\"precision so far is {}\".format(precision))\n",
    "    \n",
    "    # test, to check for overfitting\n",
    "    tacc = run_model_test(model, test_loader)\n",
    "    test_acc.append(tacc)\n",
    "    \n",
    "    acc.append(correct / (len(loader) * 16))\n",
    "    print(\"train accuracy: {}, test accuracy: {}\".format(acc[len(acc) - 1], tacc))\n",
    "    return losses\n",
    "\n",
    "# Run training loop for 10 epochsxx\n",
    "all_losses = []\n",
    "for epoch in range(100):\n",
    "    print(\"computing epoch\", epoch)\n",
    "    losses = train()\n",
    "    all_losses.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "dataset = NCaltech101Best('./data/storage/', mode='test')\n",
    "loader = PygDataLoader(dataset, batch_size=16, shuffle=True)\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            pred = out.max(dim=1)[1]\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "print(test(model,loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"trained.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_losses2 = [l.cpu().detach().numpy() for l in [**all_losses]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what to do with the losses...\n",
    "\n",
    "list_losses = []\n",
    "for losses in all_losses:\n",
    "    list_losses += losses\n",
    "\n",
    "list_losses2 = [l.cpu().detach().numpy() for l in list_losses]\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(list_losses2)), list_losses2)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57e03cdc9893d20d727156d0bac082b299e8387dff5042b3b647c6bfa1c939e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
